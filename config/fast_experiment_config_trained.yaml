stage1:
  bm25:
    b: 0.75
    k1: 1.5
  specter2:
    faiss_nlist: 100
    faiss_nprobe: 10
    model_name: allenai/specter2
  top_k: 1000
  use_bm25: true
  use_specter2: true
  use_tfidf: false
stage2:
  bi_encoder:
    fine_tuned_path: null
    model_name: allenai/scibert_scivocab_uncased
  rrf:
    k: 60
  top_k: 50
  use_bi_encoder: true
  use_colbert: false
  use_rrf: true
stage3:
  cross_encoder:
    fine_tuned_path: /hy-tmp/final_test/experiments/checkpoints/cross_encoder
    model_name: cross-encoder/ms-marco-MiniLM-L-12-v2
  top_k: 20
  use_cross_encoder: true
  use_l2r: false
training:
  scibert:
    batch_size: 16
    early_stopping_patience: 2
    epochs: 3
    learning_rate: 2e-5
    warmup_steps: 100
  train_cross_encoder: false
  train_scibert: true
  train_specter2: false
