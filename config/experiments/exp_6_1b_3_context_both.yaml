# Exp 6.1b.3: Context Enhancement - Both
# 前后文 + citation_context
# 基线: Pipeline Optimized (MRR=0.3428)
# 预期: MRR = 0.35-0.37 (+2-8%)

query_enhancement:
  context_mode: both  # both: context_before + citation_context + context_after
  use_source_paper: false

stage1:
  bm25:
    b: 0.75
    k1: 1.5
  specter2:
    faiss_nlist: 100
    faiss_nprobe: 10
    model_name: allenai/specter2
    fine_tuned_path: experiments/checkpoints/specter2
  top_k: 1000
  use_bm25: true
  use_prf: false
  use_specter2: true
  use_tfidf: true

stage2:
  bi_encoder:
    fine_tuned_path: experiments/checkpoints/scibert
    model_name: allenai/scibert_scivocab_uncased
  rrf:
    k: 60
  top_k: 50
  use_bi_encoder: true
  use_colbert: false
  use_rrf: true

stage3:
  cross_encoder:
    fine_tuned_path: experiments/checkpoints/cross_encoder
    model_name: cross-encoder/ms-marco-MiniLM-L-12-v2
  top_k: 20
  use_cross_encoder: true
  use_l2r: true
  l2r:
    model_path: experiments/checkpoints/l2r/ft/l2r_model.txt

training:
  scibert:
    batch_size: 16
    early_stopping_patience: 2
    epochs: 3
    learning_rate: 2e-5
    warmup_steps: 100
  train_cross_encoder: false
  train_l2r: false
  train_scibert: false
  train_specter2: false

