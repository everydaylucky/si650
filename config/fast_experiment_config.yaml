# 快速实验配置 - 最小可行实验方案
# 目标: 5-6小时完成，验证核心假设

# Stage 1: 初始检索
stage1:
  use_bm25: true
  use_tfidf: false  # 跳过TF-IDF节省时间
  use_specter2: true  # 使用SPECTER2 zero-shot
  top_k: 1000
  
  bm25:
    k1: 1.5
    b: 0.75
  
  specter2:
    model_name: "allenai/specter2"
    faiss_nlist: 100
    faiss_nprobe: 10

# Stage 2: 重排序
stage2:
  use_rrf: true  # RRF融合（快速）
  use_colbert: false  # 跳过ColBERT（训练时间长）
  use_bi_encoder: true  # 使用SciBERT（zero-shot和fine-tuned）
  top_k: 50
  
  rrf:
    k: 60
  
  bi_encoder:
    model_name: "allenai/scibert_scivocab_uncased"
    fine_tuned_path: null  # 将在训练后更新

# Stage 3: 最终排序
stage3:
  use_cross_encoder: true  # 使用Cross-Encoder zero-shot
  use_l2r: false  # 跳过L2R（需要所有特征）
  top_k: 20
  
  cross_encoder:
    model_name: "cross-encoder/ms-marco-MiniLM-L-12-v2"
    fine_tuned_path: null  # 只使用zero-shot，不fine-tune

# 训练配置（快速实验）
training:
  # 只训练SciBERT，跳过其他模型
  train_scibert: true
  train_specter2: false  # 跳过SPECTER2 fine-tuning
  train_cross_encoder: false  # 跳过Cross-Encoder fine-tuning
  
  scibert:
    epochs: 3  # 减少到3轮（原始5轮）
    batch_size: 16
    learning_rate: 2e-5
    warmup_steps: 100  # 减少warmup
    early_stopping_patience: 2

