#!/usr/bin/env python3
"""
æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚
"""
import json
import sys
from pathlib import Path
from collections import defaultdict

def check_data_file(file_path: str, dataset_name: str):
    """æ£€æŸ¥å•ä¸ªæ•°æ®æ–‡ä»¶"""
    print(f"\n{'='*60}")
    print(f"æ£€æŸ¥ {dataset_name}: {file_path}")
    print(f"{'='*60}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æ–‡ä»¶: {e}")
        return False
    
    # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
    if isinstance(raw_data, dict):
        if 'samples' in raw_data:
            data = raw_data['samples']
            metadata = raw_data.get('metadata', {})
            print(f"\nğŸ“‹ æ–‡ä»¶æ ¼å¼: åŒ…å«metadataçš„å¯¹è±¡")
            if metadata:
                print(f"  å…ƒæ•°æ®: {metadata}")
        else:
            print(f"âŒ å­—å…¸æ ¼å¼ä½†ç¼ºå°‘'samples'å­—æ®µ")
            return False
    elif isinstance(raw_data, list):
        data = raw_data
        print(f"\nğŸ“‹ æ–‡ä»¶æ ¼å¼: JSONæ•°ç»„")
    else:
        print(f"âŒ æœªçŸ¥çš„æ•°æ®æ ¼å¼: {type(raw_data)}")
        return False
    
    # åŸºæœ¬ç»Ÿè®¡
    total_samples = len(data)
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples:,}")
    
    if total_samples == 0:
        print("âŒ æ–‡ä»¶ä¸ºç©º")
        return False
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç»“æ„
    print(f"\nğŸ“‹ æ ·æœ¬ç»“æ„æ£€æŸ¥:")
    sample = data[0]
    print(f"  ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å­—æ®µ: {list(sample.keys())}")
    
    # å¿…éœ€å­—æ®µæ£€æŸ¥
    required_fields = {
        'citation_context': str,
        'source_paper_id': str,
        'target_paper_id': str,
        'source_paper': dict,
        'target_paper': dict
    }
    
    missing_fields = []
    type_errors = []
    
    for field, expected_type in required_fields.items():
        if field not in sample:
            missing_fields.append(field)
        elif not isinstance(sample[field], expected_type):
            type_errors.append(f"{field}: æœŸæœ› {expected_type.__name__}, å®é™… {type(sample[field]).__name__}")
    
    if missing_fields:
        print(f"  âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
    else:
        print(f"  âœ… æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨")
    
    if type_errors:
        print(f"  âŒ ç±»å‹é”™è¯¯: {type_errors}")
    else:
        print(f"  âœ… å­—æ®µç±»å‹æ­£ç¡®")
    
    # æ£€æŸ¥source_paperå’Œtarget_paperçš„ç»“æ„
    print(f"\nğŸ“„ è®ºæ–‡å¯¹è±¡ç»“æ„æ£€æŸ¥:")
    for paper_type in ['source_paper', 'target_paper']:
        if paper_type in sample:
            paper = sample[paper_type]
            paper_required = ['id', 'title', 'abstract']
            paper_missing = [f for f in paper_required if f not in paper]
            
            if paper_missing:
                print(f"  âŒ {paper_type} ç¼ºå°‘å­—æ®µ: {paper_missing}")
            else:
                print(f"  âœ… {paper_type} ç»“æ„å®Œæ•´")
                print(f"     å­—æ®µ: {list(paper.keys())}")
    
    # æ£€æŸ¥è´Ÿæ ·æœ¬ï¼ˆä»…è®­ç»ƒé›†ï¼‰
    has_negatives = 'negatives' in sample
    if dataset_name == 'train' and not has_negatives:
        print(f"\n  âš ï¸  è®­ç»ƒé›†åº”è¯¥åŒ…å«negativeså­—æ®µ")
    elif has_negatives:
        print(f"\n  âœ… åŒ…å«negativeså­—æ®µ")
        if isinstance(sample['negatives'], list):
            print(f"     è´Ÿæ ·æœ¬æ•°é‡: {len(sample['negatives'])}")
    
    # æ‰¹é‡æ£€æŸ¥ï¼ˆé‡‡æ ·æ£€æŸ¥ï¼‰
    print(f"\nğŸ” æ‰¹é‡è´¨é‡æ£€æŸ¥ï¼ˆé‡‡æ ·100ä¸ªï¼‰:")
    sample_size = min(100, total_samples)
    samples_to_check = data[:sample_size]
    
    issues = {
        'missing_citation_context': 0,
        'missing_source_paper': 0,
        'missing_target_paper': 0,
        'time_violations': 0,
        'empty_text': 0,
        'short_context': 0
    }
    
    for i, s in enumerate(samples_to_check):
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        if not s.get('citation_context'):
            issues['missing_citation_context'] += 1
        elif len(s['citation_context'].split()) < 10:
            issues['short_context'] += 1
        
        if not s.get('source_paper'):
            issues['missing_source_paper'] += 1
        if not s.get('target_paper'):
            issues['missing_target_paper'] += 1
        
        # æ£€æŸ¥æ—¶é—´ä¸€è‡´æ€§
        source_year = s.get('source_paper', {}).get('year', 0)
        target_year = s.get('target_paper', {}).get('year', 0)
        if source_year > 0 and target_year > 0 and source_year < target_year:
            issues['time_violations'] += 1
    
    # æŠ¥å‘Šé—®é¢˜
    all_ok = True
    for issue, count in issues.items():
        if count > 0:
            print(f"  âš ï¸  {issue}: {count} ä¸ªæ ·æœ¬")
            all_ok = False
    
    if all_ok:
        print(f"  âœ… é‡‡æ ·æ£€æŸ¥é€šè¿‡")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    
    # ç»Ÿè®¡å¹´ä»½åˆ†å¸ƒ
    years = []
    for s in samples_to_check:
        if s.get('source_paper', {}).get('year'):
            years.append(s['source_paper']['year'])
    
    if years:
        print(f"  æºè®ºæ–‡å¹´ä»½èŒƒå›´: {min(years)} - {max(years)}")
    
    # ç»Ÿè®¡ç±»åˆ«
    categories = defaultdict(int)
    for s in samples_to_check:
        cats = s.get('source_paper', {}).get('categories', [])
        if isinstance(cats, list):
            for cat in cats:
                categories[cat] += 1
    
    if categories:
        print(f"  ä¸»è¦ç±»åˆ« (å‰5):")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"    {cat}: {count}")
    
    # æ£€æŸ¥citation_contexté•¿åº¦
    context_lengths = [len(s.get('citation_context', '').split()) for s in samples_to_check]
    if context_lengths:
        avg_length = sum(context_lengths) / len(context_lengths)
        print(f"  å¹³å‡citation_contexté•¿åº¦: {avg_length:.1f} å•è¯")
        print(f"  æœ€çŸ­: {min(context_lengths)}, æœ€é•¿: {max(context_lengths)}")
    
    return True

def main():
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "data" / "processed"
    
    files_to_check = [
        ("train.json", "è®­ç»ƒé›†"),
        ("val.json", "éªŒè¯é›†"),
        ("test.json", "æµ‹è¯•é›†")
    ]
    
    all_ok = True
    for filename, dataset_name in files_to_check:
        file_path = data_dir / filename
        if file_path.exists():
            try:
                check_data_file(str(file_path), dataset_name)
            except Exception as e:
                print(f"\nâŒ æ£€æŸ¥ {filename} æ—¶å‡ºé”™: {e}")
                all_ok = False
        else:
            print(f"\nâš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    print(f"\n{'='*60}")
    if all_ok:
        print("âœ… æ•°æ®æ ¼å¼æ£€æŸ¥å®Œæˆ")
    else:
        print("âš ï¸  å‘ç°ä¸€äº›é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„æŠ¥å‘Š")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()

