# Exp 6.1b å®éªŒè¯´æ˜

## ğŸ“‹ source_paper å†…å®¹

æ ¹æ®å®é™…æ•°æ®æ£€æŸ¥ï¼Œ`source_paper` åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
  "id": "2010.11934",                    // è®ºæ–‡IDï¼ˆå¦‚ arXiv IDï¼‰
  "title": "mT5: A massively multilingual pre-trained text-to-text transformer",  // è®ºæ–‡æ ‡é¢˜
  "abstract": "The recent Text-to-Text Transfer Transformer (T5)...",  // è®ºæ–‡æ‘˜è¦ï¼ˆå¯èƒ½å¾ˆé•¿ï¼‰
  "categories": ["cs.CL"],              // è®ºæ–‡ç±»åˆ«
  "year": 2020                          // å‘è¡¨å¹´ä»½
}
```

### å…³é”®å­—æ®µè¯´æ˜

- **title**: è®ºæ–‡æ ‡é¢˜ï¼Œé€šå¸¸åŒ…å«è®ºæ–‡çš„æ ¸å¿ƒä¸»é¢˜
- **abstract**: è®ºæ–‡æ‘˜è¦ï¼ŒåŒ…å«è®ºæ–‡çš„ä¸»è¦å†…å®¹ã€æ–¹æ³•ã€è´¡çŒ®ç­‰
- **categories**: è®ºæ–‡ç±»åˆ«ï¼ˆå¦‚ cs.CL, cs.LGï¼‰ï¼Œè¡¨ç¤ºç ”ç©¶é¢†åŸŸ

---

## ğŸ”¬ å®éªŒè®¾è®¡è¯´æ˜

### Exp 6.1b.1: ä»…æ·»åŠ å‰åæ–‡

**æŸ¥è¯¢ç»„æˆ**:
```
query = context_before + citation_context + context_after
```

**ç¤ºä¾‹**:
```
context_before: "Recent work in NLP has shown that"
citation_context: "transformer models achieve state-of-the-art results"
context_after: "on various downstream tasks."

å®Œæ•´æŸ¥è¯¢: "Recent work in NLP has shown that transformer models achieve state-of-the-art results on various downstream tasks."
```

**ç›®çš„**: éªŒè¯å‰åæ–‡æ˜¯å¦æœ‰åŠ©äºç†è§£å¼•ç”¨å¥çš„è¯­ä¹‰

---

### Exp 6.1b.2: å‰åæ–‡ + Source Paper

**æŸ¥è¯¢ç»„æˆ**:
```
query = context_before + citation_context + context_after 
      + source_paper.title + source_paper.abstract[:200]
```

**ç¤ºä¾‹**:
```
context_before: "Recent work in NLP has shown that"
citation_context: "transformer models achieve state-of-the-art results"
context_after: "on various downstream tasks."
source_paper.title: "mT5: A massively multilingual pre-trained text-to-text transformer"
source_paper.abstract[:200]: "The recent Text-to-Text Transfer Transformer (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks..."

å®Œæ•´æŸ¥è¯¢: "Recent work in NLP has shown that transformer models achieve state-of-the-art results on various downstream tasks. mT5: A massively multilingual pre-trained text-to-text transformer The recent Text-to-Text Transfer Transformer (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks..."
```

**ç›®çš„**: 
1. å‰åæ–‡æä¾›å±€éƒ¨ä¸Šä¸‹æ–‡
2. source_paper æä¾›å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆæºè®ºæ–‡çš„ä¸»é¢˜å’Œå†…å®¹ï¼‰
3. ä¸¤è€…ç»“åˆï¼Œæä¾›æ›´å®Œæ•´çš„è¯­ä¹‰ä¿¡æ¯

---

## ğŸ’¡ ä¸ºä»€ä¹ˆæ·»åŠ  source_paperï¼Ÿ

### 1. æä¾›ä¸»é¢˜ä¸Šä¸‹æ–‡

**ä¾‹å­**:
- å¦‚æœ source_paper æ˜¯å…³äº "transformer" çš„
- é‚£ä¹ˆ citation_context å¾ˆå¯èƒ½ä¹Ÿåœ¨è®¨è®º transformer ç›¸å…³çš„å†…å®¹
- æ·»åŠ  source_paper ä¿¡æ¯å¯ä»¥å¸®åŠ©æ¨¡å‹ç†è§£å¼•ç”¨å¥çš„ä¸»é¢˜

### 2. å‡å°‘æ­§ä¹‰

**ä¾‹å­**:
- citation_context: "This method achieves good results"
- å¦‚æœä¸çŸ¥é“ source_paperï¼Œå¯èƒ½ä¸æ¸…æ¥š "This method" æŒ‡ä»€ä¹ˆ
- å¦‚æœçŸ¥é“ source_paper æ˜¯å…³äº "BERT" çš„ï¼Œå°±èƒ½ç†è§£ "This method" å¯èƒ½æŒ‡ BERT

### 3. å¢å¼ºè¯­ä¹‰åŒ¹é…

**ä¾‹å­**:
- citation_context å¯èƒ½åªæåˆ° "attention mechanism"
- source_paper çš„ abstract å¯èƒ½è¯¦ç»†æè¿°äº† "self-attention"ã€"multi-head attention" ç­‰
- æ·»åŠ  source_paper å¯ä»¥è®©æ£€ç´¢æ¨¡å‹æ›´å¥½åœ°åŒ¹é…ç›¸å…³è®ºæ–‡

---

## ğŸ“Š å®éªŒå¯¹æ¯”

| å®éªŒ | æŸ¥è¯¢ç»„æˆ | ä¿¡æ¯é‡ | é¢„æœŸæ•ˆæœ |
|------|---------|--------|---------|
| åŸºçº¿ | citation_context | æœ€å° | MRR = 0.3428 |
| Exp 6.1 | citation_context + source_paper | ä¸­ç­‰ | MRR = 0.3414 (ç•¥é™) |
| Exp 6.1b.1 | context_before + citation + context_after | ä¸­ç­‰ | MRR = 0.35-0.37 (é¢„æœŸ) |
| Exp 6.1b.2 | 6.1b.1 + source_paper | **æœ€å¤§** | MRR = 0.36-0.39 (é¢„æœŸ) |

---

## âš ï¸ æ½œåœ¨é—®é¢˜

### 1. æŸ¥è¯¢è¿‡é•¿

**é—®é¢˜**: æ·»åŠ  source_paper.abstract åï¼ŒæŸ¥è¯¢å¯èƒ½å˜å¾—å¾ˆé•¿

**è§£å†³**: 
- é™åˆ¶ abstract é•¿åº¦ï¼ˆå¦‚ 200 å­—ç¬¦ï¼‰
- åªä½¿ç”¨ abstract çš„å‰å‡ å¥

### 2. ä¿¡æ¯å™ªå£°

**é—®é¢˜**: source_paper çš„ä¿¡æ¯å¯èƒ½åŒ…å«ä¸ç›¸å…³å†…å®¹

**è§£å†³**:
- åªä½¿ç”¨ titleï¼ˆé€šå¸¸æœ€ç›¸å…³ï¼‰
- ä½¿ç”¨ abstract çš„å…³é”®å¥å­ï¼ˆéœ€è¦æå–ï¼‰

### 3. æƒé‡é—®é¢˜

**é—®é¢˜**: citation_context åº”è¯¥æ˜¯æœ€é‡è¦çš„ï¼Œä½†æ·»åŠ å¤ªå¤šå…¶ä»–ä¿¡æ¯å¯èƒ½ç¨€é‡Šå…¶é‡è¦æ€§

**è§£å†³**:
- åŠ æƒç»„åˆï¼ˆcitation_context æƒé‡æ›´é«˜ï¼‰
- æˆ–è€…åªåœ¨ç‰¹å®šé˜¶æ®µä½¿ç”¨ source_paperï¼ˆå¦‚ Stage2/Stage3ï¼‰

---

## ğŸ¯ å®æ–½å»ºè®®

### æ–¹æ¡ˆ1: ç®€å•ç»„åˆï¼ˆå½“å‰ Exp 6.1 çš„æ–¹å¼ï¼‰
```python
query = f"{context_before} {citation_context} {context_after} {source_title} {source_abstract[:200]}"
```

### æ–¹æ¡ˆ2: åŠ æƒç»„åˆï¼ˆæ›´ç²¾ç»†ï¼‰
```python
# citation_context æƒé‡æœ€é«˜
query = f"{citation_context} {context_before} {context_after} {source_title} {source_abstract[:100]}"
```

### æ–¹æ¡ˆ3: åˆ†é˜¶æ®µä½¿ç”¨ï¼ˆæ›´çµæ´»ï¼‰
```python
# Stage1: åªç”¨ citation_context + å‰åæ–‡
stage1_query = f"{context_before} {citation_context} {context_after}"

# Stage2/Stage3: æ·»åŠ  source_paper
stage2_query = f"{stage1_query} {source_title} {source_abstract[:200]}"
```

---

## ğŸ“ æ€»ç»“

**Exp 6.1b.2 = Exp 6.1b.1 + source_paper** çš„æ„æ€æ˜¯ï¼š

1. **Exp 6.1b.1**: ä½¿ç”¨å‰åæ–‡å¢å¼º citation_context
2. **Exp 6.1b.2**: åœ¨ 6.1b.1 çš„åŸºç¡€ä¸Šï¼Œå†æ·»åŠ  source_paper çš„ title å’Œ abstract

**source_paper åŒ…å«**:
- title: è®ºæ–‡æ ‡é¢˜ï¼ˆæ ¸å¿ƒä¸»é¢˜ï¼‰
- abstract: è®ºæ–‡æ‘˜è¦ï¼ˆè¯¦ç»†å†…å®¹ï¼‰
- categories: è®ºæ–‡ç±»åˆ«
- year: å‘è¡¨å¹´ä»½

**é¢„æœŸæ•ˆæœ**: æä¾›æ›´å®Œæ•´çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å¼•ç”¨å¥çš„ä¸Šä¸‹æ–‡å’Œä¸»é¢˜ã€‚

