# æ•°æ®å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“ æ•°æ®åº”è¯¥æ”¾åœ¨å“ªé‡Œï¼Ÿ

```
final_test/
â””â”€â”€ data/
    â””â”€â”€ processed/
        â”œâ”€â”€ train.json    â† è®­ç»ƒé›†ï¼ˆå¿…éœ€ï¼‰
        â”œâ”€â”€ val.json      â† éªŒè¯é›†ï¼ˆå¿…éœ€ï¼‰
        â””â”€â”€ test.json     â† æµ‹è¯•é›†ï¼ˆå¿…éœ€ï¼‰
```

## ğŸ“‹ æ•°æ®æ ¼å¼ç¤ºä¾‹

### æœ€å°ç¤ºä¾‹ï¼ˆä¸€ä¸ªæ ·æœ¬ï¼‰

```json
{
  "citation_context": "Recent work shows that transformer models...",
  "source_paper_id": "1910.10683",
  "target_paper_id": "1706.03762",
  "source_paper": {
    "id": "1910.10683",
    "title": "Exploring the Limits of Transfer Learning...",
    "abstract": "Transfer learning, where a model...",
    "categories": ["cs.LG", "cs.CL"],
    "year": 2019
  },
  "target_paper": {
    "id": "1706.03762",
    "title": "Attention Is All You Need",
    "abstract": "The dominant sequence transduction models...",
    "categories": ["cs.CL", "cs.LG"],
    "year": 2017
  }
}
```

### å®Œæ•´ç¤ºä¾‹ï¼ˆè®­ç»ƒé›†éœ€è¦è´Ÿæ ·æœ¬ï¼‰

```json
{
  "citation_context": "...",
  "source_paper_id": "...",
  "target_paper_id": "...",
  "source_paper": {...},
  "target_paper": {...},
  "negatives": [        // ä»…è®­ç»ƒé›†éœ€è¦
    {
      "id": "...",
      "title": "...",
      "abstract": "...",
      "categories": [...],
      "year": 2015
    }
  ],
  "metadata": {         // å¯é€‰
    "section": "Introduction",
    "source_year": 2019,
    "target_year": 2017
  }
}
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆæµ‹è¯•ç”¨ï¼‰

```bash
# ç¤ºä¾‹æ•°æ®å·²å‡†å¤‡å¥½
ls data/processed/example_*.json
```

### 2. ä»SI650é¡¹ç›®è½¬æ¢æ•°æ®

```bash
# å¦‚æœä½ æœ‰SI650çš„citationæ•°æ®
python scripts/prepare_data.py convert \
    ../other/citation_ground_truth.json \
    data/processed/train.json
```

### 3. æ£€æŸ¥æ•°æ®

```bash
python scripts/prepare_data.py check data/processed/train.json
```

## ğŸ“Š æ•°æ®è¦æ±‚

| æ•°æ®é›† | æ ·æœ¬æ•° | è´Ÿæ ·æœ¬æ¯”ä¾‹ | æ–‡ä»¶ä½ç½® |
|--------|--------|------------|----------|
| è®­ç»ƒé›† | 12,844 | 1:10 | `data/processed/train.json` |
| éªŒè¯é›† | 1,605 | 1:99 | `data/processed/val.json` |
| æµ‹è¯•é›† | 1,606 | 1:99 | `data/processed/test.json` |

## âœ… å¿…éœ€å­—æ®µæ£€æŸ¥æ¸…å•

æ¯ä¸ªæ ·æœ¬å¿…é¡»åŒ…å«ï¼š

- [x] `citation_context` - å¼•ç”¨ä¸Šä¸‹æ–‡æ–‡æœ¬
- [x] `source_paper_id` - æºè®ºæ–‡ID
- [x] `target_paper_id` - ç›®æ ‡è®ºæ–‡ID
- [x] `source_paper.id` - æºè®ºæ–‡ID
- [x] `source_paper.title` - æºè®ºæ–‡æ ‡é¢˜
- [x] `source_paper.abstract` - æºè®ºæ–‡æ‘˜è¦
- [x] `target_paper.id` - ç›®æ ‡è®ºæ–‡ID
- [x] `target_paper.title` - ç›®æ ‡è®ºæ–‡æ ‡é¢˜
- [x] `target_paper.abstract` - ç›®æ ‡è®ºæ–‡æ‘˜è¦

## ğŸ“ è¯¦ç»†æ–‡æ¡£

- **å®Œæ•´æ ¼å¼è¯´æ˜**: `data/DATA_FORMAT.md`
- **æ•°æ®ç›®å½•è¯´æ˜**: `data/README.md`
- **ç¤ºä¾‹æ•°æ®**: `data/processed/example_*.json`

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ—¶é—´ä¸€è‡´æ€§**: æºè®ºæ–‡å¹´ä»½å¿…é¡» â‰¥ ç›®æ ‡è®ºæ–‡å¹´ä»½
2. **æ–‡æœ¬è´¨é‡**: citation_contexté•¿åº¦ â‰¥ 10ä¸ªå•è¯
3. **æ–‡ä»¶ç¼–ç **: å¿…é¡»ä½¿ç”¨UTF-8ç¼–ç 
4. **JSONæ ¼å¼**: å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ•°ç»„

## ğŸ” éªŒè¯æ•°æ®

è¿è¡Œæµ‹è¯•ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼š

```bash
python -c "
from src.utils import load_json
data = load_json('data/processed/example_train.json')
print(f'âœ“ æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ ·æœ¬')
print(f'âœ“ ç¬¬ä¸€ä¸ªæ ·æœ¬çš„citation_context: {data[0][\"citation_context\"][:50]}...')
"
```

